{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Subvolume3DcnnEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, batch_norm_momentum, filters):\n",
    "        super().__init__()\n",
    "        strides = [1,2,2,2]\n",
    "        filter_sizes = [1] + filters\n",
    "        filter_list_pairs = list(zip(filter_sizes[:-1], filter_sizes[1:]))\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            *[nn.Sequential(\n",
    "                nn.Conv3d(chan_in, chan_out, kernel_size=3, stride=stride, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm3d(num_features=filter_, momentum=batch_norm_momentum)\n",
    "        )\n",
    "            for (chan_in, chan_out), stride, filter_ in zip(filter_list_pairs, strides, filters)])\n",
    "        self.apply(self.init_weight)\n",
    "    \n",
    "    @staticmethod\n",
    "    def init_weight(m):\n",
    "        if isinstance(m, nn.Conv3d):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv_layers(x)\n",
    "    \n",
    "class LinearInkDecoder(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(int(np.prod(input_shape)), 1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.fc(self.flatten(x)))# (B, 1) \n",
    "    \n",
    "class InkClassifier3DCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, subvolume_shape=[256,256,10], batch_norm_momentum=0.1, filters=[16, 32, 64, 128]):\n",
    "        super().__init__()\n",
    "        self.encoder = Subvolume3DcnnEncoder(batch_norm_momentum, filters)\n",
    "        self.decoder = LinearInkDecoder(self.encoder(torch.zeros((1, 1, *subvolume_shape))).shape[1:])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# HELPER FUNCTIONS\n",
    "########################################################\n",
    "\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import PIL\n",
    "import random\n",
    "import tqdm\n",
    "import gc\n",
    "\n",
    "vesuvius_data_path = 'kaggle/input/vesuvius-challenge-ink-detection/'\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def load_mask_label(fragment, rect = None, display = False):\n",
    "    \"\"\"Loads and returns mask and label for a given fragment\n",
    "    Parameters:\n",
    "      fragment (int in [1, 2, 3]): id of the fragment\n",
    "      rect (tuple): (x, y, w, h) of the subsection of the image to load\n",
    "    \"\"\"\n",
    "    mask_filepath = vesuvius_data_path+f\"train/{fragment}/mask.png\"\n",
    "    label_filepath = vesuvius_data_path+f\"train/{fragment}/inklabels.png\"\n",
    "\n",
    "    mask = cv2.imread(mask_filepath, 0) / 255.\n",
    "    label = cv2.imread(label_filepath, 0) /255.\n",
    "    if rect is not None:\n",
    "        mask = mask[rect[1]:rect[1]+rect[3], rect[0]:rect[0]+rect[2]]\n",
    "        label = label[rect[1]:rect[1]+rect[3], rect[0]:rect[0]+rect[2]]\n",
    "    return mask, label\n",
    "\n",
    "def load_image_stack(fragment, Z_START, Z_DIM, rect = None, folder='train', display_all = False, display_one = False):\n",
    "    root_filepath = vesuvius_data_path+f\"{folder}/{fragment}/surface_volume/\"\n",
    "    tif_filepaths = [root_filepath+ x for x in sorted(os.listdir(root_filepath))[:-4]]\n",
    "    layers_to_use = tif_filepaths[Z_START:Z_START+Z_DIM]\n",
    "    image_stack = []\n",
    "    for filepath in layers_to_use:\n",
    "        loaded_img = torch.Tensor(cv2.imread(filepath, 0) / 65535.0).float().to(device)\n",
    "        if rect is None:\n",
    "            image_stack.append(loaded_img)\n",
    "        else:\n",
    "            image_stack.append(loaded_img[rect[1]:rect[1]+rect[3], rect[0]:rect[0]+rect[2]])\n",
    "    return torch.stack(image_stack, dim=0)\n",
    "    \n",
    "\n",
    "def get_pixels(mask, img_size, stride = 0):\n",
    "    \"\"\"Returns pixels inside rectangle and mask\n",
    "    Parameters:\n",
    "        mask (np.array): mask of the image\n",
    "        img_size (int): size of the image\n",
    "        stride (int): how many pixels to skip (e.g. 0 means all pixels, 3 means a gap of 3 horiontally and vertically between pixels)\n",
    "    \"\"\"\n",
    "    radius = int(img_size//2)\n",
    "    # Create a Boolean array mask of the same shape as the mask, initially all True\n",
    "    not_border = np.zeros(mask.shape, dtype=bool)\n",
    "    not_border[radius:mask.shape[0]-radius, radius:mask.shape[1]-radius] = True\n",
    "    arr_mask = np.array(mask) * not_border\n",
    "\n",
    "    if stride !=0:\n",
    "        assert stride % 2 == 1, \"stride has to be an odd number!\"\n",
    "        sparse_mask = np.zeros(mask.shape, dtype=bool)\n",
    "        sparse_mask[::stride, ::stride] = True\n",
    "        return np.argwhere(sparse_mask*arr_mask)\n",
    "    \n",
    "    return np.argwhere(arr_mask)\n",
    "\n",
    "class SubvolumeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_stack, label, pixels, img_size):\n",
    "        self.image_stack = image_stack\n",
    "        self.label = torch.Tensor(label).float()\n",
    "        self.pixels = pixels\n",
    "        self.radius = int(img_size//2)\n",
    "    def __len__(self):\n",
    "        return len(self.pixels)\n",
    "    def __getitem__(self, index):\n",
    "        y, x = self.pixels[index] \n",
    "        subvolume = self.image_stack[:, y-self.radius:y+self.radius, x-self.radius:x+self.radius]\n",
    "        # ink_label = self.labels[y-self.radius:y+self.radius, x-self.radius:x+self.radius] uncomment this line to use a segmentation model\n",
    "        ink_label = self.label[y, x]\n",
    "        return subvolume, ink_label\n",
    "    \n",
    "#Subvolume patches to train on\n",
    "#[fragment_id, subvolume]\n",
    "#x, y, w, h\n",
    "test_patches = {1: [2000, 400, 2500, 1000]}\n",
    "\n",
    "#x, y, w, h\n",
    "train_patches = [\n",
    "    [1, [200, 1500, 4500, 6500]],\n",
    "    [2, [500, 500, 4500, 5000]],\n",
    "    [2, [500, 5500, 4000, 5000]],\n",
    "    [2, [4500, 5500, 4500, 5000]],\n",
    "    [2, [500, 10500, 8500, 4000]],\n",
    "    [3, [200, 800, 5000, 6500]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward pass: torch.Size([5, 1])\n",
      "Number of params: 307,841\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "img_size = 64 # 256\n",
    "Z_START = 27\n",
    "Z_DIM = 10\n",
    "\n",
    "batch_size = 32\n",
    "lr = 3e-3 \n",
    "\n",
    "model = InkClassifier3DCNN(subvolume_shape=[img_size, img_size, 10], filters = [16,32,64,128]).to(device)\n",
    "xb = torch.rand((5,1,img_size,img_size,10)).to(device)\n",
    "print(\"forward pass:\", model(xb).shape)\n",
    "print(f\"Number of params: {(sum(p.numel() for p in model.parameters() if p.requires_grad)):,}\")\n",
    "\n",
    "total_training_steps = 100000 # (really means 50,000*batch_size steps)\n",
    "subvolume_training_steps = 2000\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, total_steps=total_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- New dataloader section --------------------\n",
      "## Fragment 2, patch [500, 500, 4500, 5000], Taken 40.09s to load ##\n",
      "training step: 2000 | Dataloader loss: 0.4258 | dataloader time: 1677.17s | t_start: 1717.27s | deleted: 1110\n",
      "## Fragment 2, patch [500, 5500, 4000, 5000], Taken 29.48s to load ##\n",
      "training step: 4000 | Dataloader loss: 0.5203 | dataloader time: 1684.83s | t_start: 3431.59s | deleted: 0\n",
      "## Fragment 2, patch [500, 10500, 8500, 4000], Taken 55.07s to load ##\n",
      "training step: 6000 | Dataloader loss: 0.4614 | dataloader time: 1875.85s | t_start: 5362.55s | deleted: 0\n",
      "## Fragment 3, patch [200, 800, 5000, 6500], Taken 9.94s to load ##\n",
      "training step: 8000 | Dataloader loss: 0.3551 | dataloader time: 1737.29s | t_start: 7109.78s | deleted: 0\n",
      "## Fragment 2, patch [4500, 5500, 4500, 5000], Taken 32.98s to load ##\n",
      "training step: 10000 | Dataloader loss: 0.4597 | dataloader time: 1464.58s | t_start: 8607.35s | deleted: 0\n",
      "## Fragment 1, patch [200, 1500, 4500, 6500], Taken 9.00s to load ##\n",
      "training step: 12000 | Dataloader loss: 0.4943 | dataloader time: 1368.12s | t_start: 9984.47s | deleted: 0\n",
      "-------------------- New dataloader section --------------------\n",
      "## Fragment 3, patch [200, 800, 5000, 6500], Taken 6.95s to load ##\n",
      "training step: 14000 | Dataloader loss: 0.3285 | dataloader time: 1377.69s | t_start: 11369.11s | deleted: 0\n",
      "## Fragment 2, patch [500, 5500, 4000, 5000], Taken 24.19s to load ##\n",
      "training step: 16000 | Dataloader loss: 0.4874 | dataloader time: 1363.99s | t_start: 12757.30s | deleted: 0\n",
      "## Fragment 2, patch [500, 500, 4500, 5000], Taken 25.22s to load ##\n",
      "training step: 18000 | Dataloader loss: 0.3722 | dataloader time: 1513.43s | t_start: 14295.96s | deleted: 0\n",
      "## Fragment 1, patch [200, 1500, 4500, 6500], Taken 8.87s to load ##\n",
      "training step: 20000 | Dataloader loss: 0.4180 | dataloader time: 1344.47s | t_start: 15649.30s | deleted: 0\n",
      "## Fragment 2, patch [4500, 5500, 4500, 5000], Taken 22.01s to load ##\n",
      "training step: 22000 | Dataloader loss: 0.4035 | dataloader time: 1315.88s | t_start: 16987.19s | deleted: 0\n",
      "## Fragment 2, patch [500, 10500, 8500, 4000], Taken 35.58s to load ##\n",
      "training step: 24000 | Dataloader loss: 0.4145 | dataloader time: 1353.58s | t_start: 18376.37s | deleted: 0\n",
      "-------------------- New dataloader section --------------------\n",
      "## Fragment 2, patch [500, 500, 4500, 5000], Taken 23.27s to load ##\n",
      "training step: 26000 | Dataloader loss: 0.3678 | dataloader time: 1297.61s | t_start: 19697.26s | deleted: 0\n",
      "## Fragment 2, patch [4500, 5500, 4500, 5000], Taken 23.39s to load ##\n",
      "training step: 28000 | Dataloader loss: 0.3941 | dataloader time: 1309.41s | t_start: 21030.07s | deleted: 0\n",
      "## Fragment 3, patch [200, 800, 5000, 6500], Taken 6.58s to load ##\n",
      "training step: 30000 | Dataloader loss: 0.3218 | dataloader time: 1338.53s | t_start: 22375.18s | deleted: 0\n",
      "## Fragment 1, patch [200, 1500, 4500, 6500], Taken 8.73s to load ##\n",
      "training step: 32000 | Dataloader loss: 0.4233 | dataloader time: 1311.95s | t_start: 23695.85s | deleted: 0\n",
      "## Fragment 2, patch [500, 10500, 8500, 4000], Taken 33.55s to load ##\n",
      "training step: 34000 | Dataloader loss: 0.4143 | dataloader time: 1332.98s | t_start: 25062.39s | deleted: 0\n",
      "## Fragment 2, patch [500, 5500, 4000, 5000], Taken 46.60s to load ##\n",
      "training step: 36000 | Dataloader loss: 0.4645 | dataloader time: 1315.49s | t_start: 26424.49s | deleted: 0\n",
      "-------------------- New dataloader section --------------------\n",
      "## Fragment 3, patch [200, 800, 5000, 6500], Taken 6.98s to load ##\n",
      "training step: 38000 | Dataloader loss: 0.3076 | dataloader time: 1340.52s | t_start: 27771.99s | deleted: 0\n",
      "## Fragment 2, patch [500, 10500, 8500, 4000], Taken 23.43s to load ##\n",
      "training step: 40000 | Dataloader loss: 0.4174 | dataloader time: 1316.69s | t_start: 29112.12s | deleted: 0\n",
      "## Fragment 2, patch [500, 500, 4500, 5000], Taken 23.74s to load ##\n",
      "training step: 42000 | Dataloader loss: 0.3462 | dataloader time: 1298.82s | t_start: 30434.70s | deleted: 0\n",
      "## Fragment 2, patch [500, 5500, 4000, 5000], Taken 22.61s to load ##\n",
      "training step: 44000 | Dataloader loss: 0.4274 | dataloader time: 1301.36s | t_start: 31758.68s | deleted: 0\n",
      "## Fragment 2, patch [4500, 5500, 4500, 5000], Taken 21.95s to load ##\n",
      "training step: 46000 | Dataloader loss: 0.3857 | dataloader time: 1305.80s | t_start: 33086.45s | deleted: 0\n",
      "## Fragment 1, patch [200, 1500, 4500, 6500], Taken 8.26s to load ##\n",
      "training step: 48000 | Dataloader loss: 0.4079 | dataloader time: 1308.18s | t_start: 34402.89s | deleted: 0\n",
      "-------------------- New dataloader section --------------------\n",
      "## Fragment 2, patch [500, 10500, 8500, 4000], Taken 45.15s to load ##\n",
      "training step: 50000 | Dataloader loss: 0.3955 | dataloader time: 1303.45s | t_start: 35751.51s | deleted: 0\n",
      "## Fragment 2, patch [500, 5500, 4000, 5000], Taken 31.61s to load ##\n",
      "training step: 52000 | Dataloader loss: 0.4155 | dataloader time: 1304.09s | t_start: 37087.23s | deleted: 0\n",
      "## Fragment 2, patch [4500, 5500, 4500, 5000], Taken 23.01s to load ##\n",
      "training step: 54000 | Dataloader loss: 0.3771 | dataloader time: 1295.70s | t_start: 38405.95s | deleted: 0\n",
      "## Fragment 2, patch [500, 500, 4500, 5000], Taken 23.07s to load ##\n",
      "training step: 56000 | Dataloader loss: 0.3285 | dataloader time: 1304.21s | t_start: 39733.24s | deleted: 0\n",
      "## Fragment 1, patch [200, 1500, 4500, 6500], Taken 8.57s to load ##\n",
      "training step: 58000 | Dataloader loss: 0.3798 | dataloader time: 1341.89s | t_start: 41083.70s | deleted: 0\n",
      "## Fragment 3, patch [200, 800, 5000, 6500], Taken 7.00s to load ##\n",
      "training step: 60000 | Dataloader loss: 0.2991 | dataloader time: 1410.81s | t_start: 42501.51s | deleted: 0\n",
      "-------------------- New dataloader section --------------------\n",
      "## Fragment 2, patch [500, 10500, 8500, 4000], Taken 27.37s to load ##\n",
      "training step: 62000 | Dataloader loss: 0.4075 | dataloader time: 2987.32s | t_start: 45516.23s | deleted: 0\n",
      "## Fragment 1, patch [200, 1500, 4500, 6500], Taken 18.22s to load ##\n",
      "training step: 64000 | Dataloader loss: 0.3715 | dataloader time: 2062.75s | t_start: 47597.21s | deleted: 0\n",
      "## Fragment 2, patch [500, 5500, 4000, 5000], Taken 32.40s to load ##\n",
      "training step: 66000 | Dataloader loss: 0.4147 | dataloader time: 1870.19s | t_start: 49499.82s | deleted: 0\n",
      "## Fragment 3, patch [200, 800, 5000, 6500], Taken 10.52s to load ##\n",
      "training step: 68000 | Dataloader loss: 0.2818 | dataloader time: 1717.15s | t_start: 51227.49s | deleted: 0\n",
      "## Fragment 2, patch [500, 500, 4500, 5000], Taken 28.88s to load ##\n",
      "training step: 70000 | Dataloader loss: 0.3264 | dataloader time: 1675.30s | t_start: 52931.69s | deleted: 0\n",
      "## Fragment 2, patch [4500, 5500, 4500, 5000], Taken 34.41s to load ##\n",
      "training step: 72000 | Dataloader loss: 0.3714 | dataloader time: 1638.21s | t_start: 54604.33s | deleted: 0\n",
      "-------------------- New dataloader section --------------------\n",
      "## Fragment 2, patch [500, 10500, 8500, 4000], Taken 31.04s to load ##\n",
      "training step: 74000 | Dataloader loss: 0.3807 | dataloader time: 1602.74s | t_start: 56238.12s | deleted: 0\n",
      "## Fragment 2, patch [500, 5500, 4000, 5000], Taken 56.79s to load ##\n",
      "training step: 76000 | Dataloader loss: 0.3908 | dataloader time: 1832.57s | t_start: 58127.49s | deleted: 0\n",
      "## Fragment 2, patch [500, 500, 4500, 5000], Taken 51.50s to load ##\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Documents\\Personal\\Development Projects\\Kaggle_Competitions\\Vesuvius_Ink\\BaselineModel.ipynb Cell 4\u001b[0m in \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/Personal/Development%20Projects/Kaggle_Competitions/Vesuvius_Ink/BaselineModel.ipynb#W4sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/Personal/Development%20Projects/Kaggle_Competitions/Vesuvius_Ink/BaselineModel.ipynb#W4sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Documents/Personal/Development%20Projects/Kaggle_Competitions/Vesuvius_Ink/BaselineModel.ipynb#W4sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/Personal/Development%20Projects/Kaggle_Competitions/Vesuvius_Ink/BaselineModel.ipynb#W4sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m scheduler\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/Personal/Development%20Projects/Kaggle_Competitions/Vesuvius_Ink/BaselineModel.ipynb#W4sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m dataloader_loss\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\NLP_Heat\\Py\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:69\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     68\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\NLP_Heat\\Py\\lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\NLP_Heat\\Py\\lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\NLP_Heat\\Py\\lib\\site-packages\\torch\\optim\\adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[0;32m    133\u001b[0m         group,\n\u001b[0;32m    134\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    139\u001b[0m         state_steps)\n\u001b[1;32m--> 141\u001b[0m     adam(\n\u001b[0;32m    142\u001b[0m         params_with_grad,\n\u001b[0;32m    143\u001b[0m         grads,\n\u001b[0;32m    144\u001b[0m         exp_avgs,\n\u001b[0;32m    145\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    146\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    147\u001b[0m         state_steps,\n\u001b[0;32m    148\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    149\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    150\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    151\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    152\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    153\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    154\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    155\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    156\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    157\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    158\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    159\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\NLP_Heat\\Py\\lib\\site-packages\\torch\\optim\\adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 281\u001b[0m func(params,\n\u001b[0;32m    282\u001b[0m      grads,\n\u001b[0;32m    283\u001b[0m      exp_avgs,\n\u001b[0;32m    284\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    285\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    286\u001b[0m      state_steps,\n\u001b[0;32m    287\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    288\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    289\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    290\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    291\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    292\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    293\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    294\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    295\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    296\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    297\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[1;32mc:\\NLP_Heat\\Py\\lib\\site-packages\\torch\\optim\\adam.py:391\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    389\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m    390\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39;49m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m    393\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_step = 0\n",
    "t_start = time.time()\n",
    "model.train()\n",
    "while training_step < total_training_steps:\n",
    "    print(\"-\"*20, \"New dataloader section\", \"-\"*20)\n",
    "    random.shuffle(train_patches)\n",
    "    for fragment, patch in train_patches:\n",
    "        if training_step >= total_training_steps:\n",
    "            break\n",
    "        t_load = time.time()\n",
    "\n",
    "        mask, label = load_mask_label(fragment, rect = patch)\n",
    "        image_stack = load_image_stack(fragment, Z_START, Z_DIM, rect = patch)\n",
    "        pixels = get_pixels(mask, img_size)\n",
    "        del mask\n",
    "        dataset = SubvolumeDataset(image_stack, label, pixels, img_size)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        print(f\"## Fragment {fragment}, patch {patch}, Taken {time.time() - t_load:.2f}s to load ##\")\n",
    "        dataloader_loss = []\n",
    "        dataloader_steps = 0\n",
    "        t_dataloader = time.time()\n",
    "        for i, (subvolumes, ink_labels) in enumerate(dataloader):\n",
    "            if dataloader_steps >= subvolume_training_steps or training_step >= total_training_steps:\n",
    "                break\n",
    "            ink_labels = ink_labels.to(device)\n",
    "            logits = model(subvolumes.permute(0, 2, 3, 1).unsqueeze(dim=1))\n",
    "            loss = loss_fn(logits, ink_labels.unsqueeze(dim=1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            dataloader_loss.append(loss.item())\n",
    "            training_step += 1\n",
    "            dataloader_steps += 1\n",
    "        del image_stack, label, dataset, dataloader\n",
    "        deleted_elements = gc.collect()\n",
    "        print(f\"training step: {training_step} | Dataloader loss: {np.array(dataloader_loss).mean():.4f} | dataloader time: {time.time() - t_dataloader:.2f}s | t_start: {time.time()-t_start:.2f}s | deleted: {deleted_elements}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(fragment = 'a', stride = 99, patch = None):\n",
    "    \"\"\"Return predictions for the test fragments\n",
    "    Parameters:\n",
    "        fragment: str, 'a' or 'b'\n",
    "        stride: int, stride for the sliding window\n",
    "    \"\"\"\n",
    "    t_load = time.time()\n",
    "    if fragment in ['a', 'b']: # test set\n",
    "        mask_filepath = vesuvius_data_path+f\"test/{fragment}/mask.png\"\n",
    "        mask = torch.Tensor(cv2.imread(mask_filepath, 0) / 255.)\n",
    "        image_stack = load_image_stack(fragment, Z_START, Z_DIM, folder = 'test')\n",
    "    elif fragment in [1, 2, 3]:\n",
    "        mask, label = load_mask_label(fragment, rect = patch)\n",
    "        image_stack = load_image_stack(fragment, Z_START, Z_DIM, rect = patch)\n",
    "        \n",
    "    test_pixels = get_pixels(mask, img_size, stride=stride)\n",
    "    print(f\"Mask size {mask.shape}, Striding by {stride}, we have {len(test_pixels)} pixels to test | Time to load: {time.time() - t_load:.2f}s\")\n",
    "    test_dataset =  SubvolumeDataset(image_stack, mask, test_pixels, img_size)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    print(f\"Length of test dataloader: {len(test_dataloader)} batches of size {batch_size}\")\n",
    "\n",
    "    t_generate = time.time()\n",
    "    output = torch.zeros_like(torch.Tensor(mask)).float()\n",
    "    model.eval()\n",
    "    radius = stride//2\n",
    "    with torch.no_grad():\n",
    "        for i, (subvolumes, _) in enumerate(test_dataloader):   \n",
    "            for j, value in enumerate(model(subvolumes.to(device).permute(0, 2, 3, 1).unsqueeze(dim=1))):\n",
    "                y, x = test_pixels[i*batch_size+j]\n",
    "                output[y-radius:y+radius+1, x-radius: x+radius+1] = value\n",
    "                # output[y, x] = value\n",
    "    print(f\"Generated pixels!! Time taken: {time.time() - t_generate:.2f}s\")\n",
    "    return output.cpu()\n",
    "\n",
    "def dice_coef_torch(preds, targets, beta=0.5, smooth=1e-5):\n",
    "    preds = np.array(preds)\n",
    "    targets = np.array(targets)\n",
    "    y_true_count = targets.sum()\n",
    "    ctp = preds[targets==1].sum() # targets==1, preds==1\n",
    "    cfp = preds[targets==0].sum() # targets==0, preds==1\n",
    "    beta_squared = beta * beta\n",
    "    c_precision = ctp / (ctp + cfp + smooth)\n",
    "    c_recall = ctp / (y_true_count + smooth)\n",
    "    dice = (1 + beta_squared) * (c_precision * c_recall) / (beta_squared * c_precision + c_recall + smooth)\n",
    "    return round(dice, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Deleted:\", gc.collect())\n",
    "\n",
    "fragment=1\n",
    "train_pred = generate_predictions(fragment = fragment, stride = 19, patch = test_patches[fragment])\n",
    "mask, label = load_mask_label(fragment, rect = test_patches[fragment])\n",
    "plt.imshow(train_pred, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_to_test = 200\n",
    "best_dice = 0\n",
    "best_threshold = 0\n",
    "for threshold in torch.rand(samples_to_test):\n",
    "    binary_pred = train_pred.clone().gt(threshold)\n",
    "    dice_coef = dice_coef_torch(binary_pred, label, beta = 0.5)\n",
    "    if dice_coef > best_dice: \n",
    "        best_dice = dice_coef\n",
    "        best_threshold = threshold\n",
    "        print(f\"Threshold: {threshold.item():.5f} | Dice: {dice_coef:.5f}\")\n",
    "\n",
    "binary_pred = train_pred.clone().gt(best_threshold)\n",
    "plt.imshow(binary_pred, cmap = \"gray\")\n",
    "plt.imshow(label, cmap = 'gray', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gc.collect())\n",
    "\n",
    "def rle(img, thr=best_threshold):\n",
    "    img = np.array(img)\n",
    "    flat_img = img.flatten()\n",
    "    flat_img = np.where(flat_img > thr, 1, 0).astype(np.uint8)\n",
    "\n",
    "    starts = np.array((flat_img[:-1] == 0) & (flat_img[1:] == 1))\n",
    "    ends = np.array((flat_img[:-1] == 1) & (flat_img[1:] == 0))\n",
    "    starts_ix = np.where(starts)[0] + 2\n",
    "    ends_ix = np.where(ends)[0] + 2\n",
    "    lengths = ends_ix - starts_ix\n",
    "\n",
    "    return starts_ix, lengths\n",
    "\n",
    "pred_list = []\n",
    "for fragment in ['a', 'b']:\n",
    "    train_pred = generate_predictions(fragment = fragment, stride = 19)\n",
    "    plt.imshow(train_pred.gt(best_threshold), cmap=\"gray\")\n",
    "    plt.show()\n",
    "    starts_ix, lengths = rle(train_pred, thr=0.4)\n",
    "    inklabels_rle = \" \".join(map(str, sum(zip(starts_ix, lengths), ())))\n",
    "    pred_list.append({\"Id\": str(fragment).split(\"/\")[-1], \"Predicted\": inklabels_rle})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final cell:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(pred_list).to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
